## RNN (Recurrent Networks) 순환 신경망
- 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델 중 하나로, 가장 기본적인 시퀀스 모델임. 지금은 쓰지 않는다. 다른 네트워크에 기본이 됨.
- RNN의 문제점을 개선한 모델이 LSTM
- one to one: 가장 기본적인 신경망 구조. 아무 것도 섞지 않은 상태. Vanilla neural networks. 하나의 입력과 하나의 출력 구조. 순환 신경망이 아님. 입력층-히든층-출력층. 
- RNN 구조 : one to many, many to one, many to many 구조가 있음.
- one to many : 사진 한 장을 주고, 여러 문장으로 설명하라는 유형의 문제. 하나의 입력과 여러 개의 출력. 입력 층에서 RNN 셀로 이동한 후, 다른 RNN 셀로 또 이동하게 됨.(예) Image captioning: image -> sequence of words.
- many to one : 여러 개의 입력과 하나의 출력. (예) Senciment classification. sequence of words -> sentiment. 댓글에 대한 감성 분류를 긍정/부정으로 하는 경우, 주식 종가 예측.
- many to many : (예) 기계번역 Machine translation. Seq of words -> seq of words. 프레임 단위의 영상 분류. Video classification on frame level(4차원 구조). 
- RNN 구조에서 사용되는 활성함수 : tanh 함수.
- 이전 RNN셀의 결과(old state)가 현재 시점의 입력값과 함께 tanh함수(하이퍼볼릭탄젠트함수)에 의해 다음 RNN셀(new state)에 영향을 주게 됨.
- Character-level language model : 문자 단위로 다음에 나올 문자를 예측하는 언어 모델
- vocabulary : 코퍼스를 구성하는 전체 단어들의 집합
- vocabulary의 수가 입력층의 차원을 결정하게 된다
- RNN의 장기 의존성 문제(The problem of long-term dependencies): 출력 결과가 이전의 개선구조에 의존할 수 밖에 없음. 짧은 시퀀스에 대해서만 효과가 발휘됨. 긴 시퀀스에 대해서는 이전의 정보가 뒤쪽으로 충분히 전달되지 못하는 현상이 발생함. 정보의 손실 발생. 만약 앞에 있는 정보가 중요한 정보였다고 해도, 이 정보가 거의 영향력을 발휘할 수 없게 됨. 시간이 흘러감에 따라 긴 시퀀스에서는 앞쪽의 정보의 손실이 발생할 수 밖에 없음. 예측 결과도 제대로 나오지 못하게 됨.

## LSTM(Long Short Term memory)
- RNN의 단점(장기의존성 문제)을 보완하기 위한 구조
- 히든계층에 있는 셀에 입력 게이트, 망각 게이트, 출력 게이트가 추가되어, 기억해야 할 것과 버려야 할 것을 판단하여 필요한 정보는 취하고 불필요한 정보는 버리는 역할을 하게됨. 히든 계층안에서 계산해야하는 사항이 늘어남. 복잡해짐. 
- https://colah.github.io/posts/2015-08-Understanding-LSTMs/
- RNN과 비교해, 긴 시퀀스의 입력을 처리할 때 성능이 월등히 좋음.
- 셀을 쭉 있는 선이 하나 더 추가됨.
- Cell state:  셀과 셀 사이를 통과하는 선. 모든 셀에 걸쳐서 연결되어 있는 선. 컨베이어 벨트와 유사함. 정보가 이 선을 따라 이동. 게이트라고 불리는 구조에 의해 세심하게 조정됨.
- 게이트: 상황에 따라, 상황에 맞춰 선택적으로 정보를 전달한다. 
- forget gate layer, input gate layer 


