## 인공 신경망 모형
- 인간의 뉴런의 자극전달 과정에서 아이디어를 착안하여 발생한 머신러닝 알고리즘
- 인간의 뉴런은 시냅스를 통하여 다른 뉴런으로부터 자각을 전달받고 시냅스를 통하여 다른 뉴런에게 자극을 전달하는 과정을 통해서 학습을 진행.
- input(입력레이어)- hidden layer - outputs(출력레이어)
- input -> hidden layer로 전달되는 선이 weight(bias 변수)
- 활성화 함수: 연산 결과에 대해 다음 뉴런으로 전달할 것인지, 말 것인지, 전달 한다면 어떤 값을 전달할 것인지를 판단하는 역할을 함. Sigmoid, Relu, Leaky Relu, Step, Tanh, ...
- 양수인 경우 1, 그렇지 않으면 -1 출력.
- 퍼셉트론은 inputs/weights/transfer function/activation function으로 이루어짐
- 입력층 - 소프트 맥스 - 출력층 : 로지스틱 리그레션을 이용한 분류기가 여러개 있을 때 다중분류가 가능해짐. 다중 분류기에서 출력된 값을 score 또는 logit이라고 부르며, 이 값들이 소프트맥스로 들어가면 확률로 변환되어 나온다. 
- 다중 분류기의 구조에서 hidden layer가 추가되면 얕은 신경망 알고리즘이 됨. hidden layer의 수가 늘어나면 깊은 신경망 알고리즘. 입력 데이터가 hidden layer의 모든 노드(뉴런)에 다 연결된다. '입력데이터수 * hidden layer 뉴런 수 + 상수'의 파라미터가 존재하게 됨. hidden layer에서 출력층으로 연결되는 가중치도 존재함.
- hidden layer : 독립변수들의 추상화된 특징이라고 말 할 수 있음. 또한 이 추상화된 특징이 출력층에 미치는 영향을 연결함. 이 결과를 시그모이드 함수에 넣어 0~1사이 값을 얻게 됨.
- 신경망은 지도학습에 해당됨
- 완전 연결 신경망(fully-connected layers): 모든 노드들이 1:1로 연결된 구조
- 신경망으로 회귀와 분류 작업을 해 낼 수 있음
- 경사하강법(Gradient descent) 등 모델의 성능을 높이기 위한 여러가지 방안이 강구되고 있음
- 어떤 경우에 hidden layer를 추가하는 것이 좋은가? 
- 학습과정은? 주어진 데이터가 입력층에 들어가서, 가중치를 랜덤값으로 준다. (경사하강법을 이용해서 cost가 가장 낮은 값을 찾아서 줄 수 있음) 훈련할 때는 이렇게 랜덤값으로 분류를 해서, 나온 결과가 잘못 나온다면, 가중치를 변경해서 다시 훈련을 한다. 결과에 더 크게 영향을 미친 가중치의 값을 변경해서 훈련을 함. 영향력을 알아내기 위해 미분을 한다. 각 가중치의 변수들이 결과에 대해 어느정도 영향을 미쳤는지에 대한 결과가 나온다. (합성함수의 미분에 됨) 결과에 대해 역방향으로 나아가면서 가중치를 업데이트 함. Backward propagation. 순방향으로 나아가면서 학습하고(forward propagation), 에러가 발생하면 역방향으로 나아가면서 가중치를 업데이트 한다. 데이터 하나를 순전파하고 다시 역전파 하는 것을 1 batch-size라고 함. 한 건의 데이터를 읽을 때 마다 가중치가 업데이트 됨. 10 batch-size는 데이터를 읽을 때 10개 단위로 읽어서 학습을 하고 가중치를 업데이트 함을 의미함. 텐서플로우. 학습이 한번 이루어지면 이를 1 epoch이라고 부름. 1000개의 데이터를 10 batch-size로 학습한다면, 총 100번의 epoch가 일어나게 됨.
- 경사하강법(Gradient descent) : 최적의 가중치를 찾는 방법 중 하나. cost가 가장 낮은 지점의 가중치를 찾는 것. 최적의 w를 찾는 것.
- 딥러닝에서 사용하는 옵티마이저는 훈련하는 과정에서 조금씩 기울기가 낮아지는 방향으로 이루어진다. 

## 딥러닝
- hidden layer가 2개 이상이 있는 경우를 말한다.
- 자연어 처리, 이미지 인식 등 인간의 수준에 거의 근접했거나 능가한 영역
