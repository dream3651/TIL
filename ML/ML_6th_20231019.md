## 앙상블 기법

- 의사결정트리로 만든 모델은 성능이 낮고, 과적합 우려도 큼. 이런 취약점을 해결하기 위한 방법으로 앙상블 기법이 나오게 됨.
- 여러 개의 분류기를 생성하고, 그 예측을 결합하여 더욱 정확한 예측을 도출하는 기법
- 강력한 하나의 모델을 사용하는 것 보다, 약한 모델 여러 개를 조합하여 사용해 강한 모델을 생성함. 한 가지 알고리즘으로 만드는 것이 아니고 다양한 알고리즘으로 만든다. 약한 모델들을 연결해서 학습을 시키면 오류가 나오더라도 정확도가 향상되는 결과 도출.

1. 보팅(Voting): 하드보팅과 소프트 보팅이 있다.
- 하드보팅: 다수결 원칙과 닮아 있음. 다수의 분류 모델에서 최대 득표를 받은 '예측값'으로 결론을 도출하는 것.
- 소프트보팅: 각 분류값별 '확률'을 합한 값을 점수로 사용해 최대 점수를 가진 분류값을 결론으로 도출. 하드보팅보다 성능이 좋다.
- 하드보팅과 소프트보팅의 결과가 다르게 나올 수 있음.
- 보팅 방식은 서로 다른 알고리즘을 사용해서 모델을 생성하고, 각각의 모델에서 예측한 결과를 취합해서 보팅을 실시하여 최종 예측 결과를 도출한다. 이때 사용하는 데이터 셋은 '전체 데이터'임.

2. 배깅(Bagging: Bootstrap + aggregation)
- 훈련 데이터 셋에서 중복을 허용한 상태에서 샘플(표본)을 여러 번 추출해서 모델을 학습시켜, 그 결과물을 집계함. 
- cf. 페이스팅(Pasting): 중복을 허용하지 않는 방식. 편향성이 없다.
- 전체 데이터셋에서 '중복을 허용해서 샘플링을 여러 개' 한 뒤, '같은 알고리즘'을 사용하여 여러 개의 샘플로 모델을 만든 뒤 보팅을 하여 최종 예측값을 도출한다.
- 랜덤 포레스트(Random Forest): 다수의 결정 트리들을 배깅해서 예측을 실행하는 앙상블 기법. 정확도가 상당히 높다. 샘플을 여러번 추출하여 모델을 학습할 때 의사결정트리 여러 개를 만들어서, 각각의 결과를 집계해서 판단하는 모델. 트리를 몇 개로 할 것인지를 설정해야 함. 또한, 각각의 나무의 깊이(depth)는 얼마로 할 것인지, 데이터에서 몇 개의 속성을 이용해서 트리를 구성할 지 등도 설정해야 함. 디폴트 값으로 하면 과적합모델이 되므로 조정해 줘야 한다. 이런 속성을 통해 과대적합 방지 가능.
- 부트스트랩? 데이터를 약간 편향되도록(중복을 허용하기 때문) 샘플링 하는 기법. 약간의 편향을 주면 모델의 성능이 올라가고, 과대적합 문제도 해소 할 수 있다. 
- 편향(Bias: 정답과 예측의 차이. 예측값들의 평균에서 정답을 뺀 후 제곱함)과 분산(variance: 각 예측값과 예측값들의 평균 간의 차이에 대해 평균을 계산하여 제곱함. 예측값들의 흩어진 정도를 나타냄): 모델이 출력한 예측값의 특징, 모습, 형태를 이해할 때 사용하는 개념. 편향이 작고 분산이 낮은 것이 이상적인 모습이다. 편향과 분산은 트레이드 오프 관계.
정정선을 찾으려면 먼저 그래프를 그려봐야 함.
- 어그리게이팅: 여러 분류 모델이 예측한 값들을 조합해서 하나의 결론을 도출하는 과정. 결론은 보팅을 통해 결정
- 각 트리의 구성을 다르게 함으로써, 일반화 성능을 향상시킬 수 있음.

3. 부스팅(Boosting)
- 이전 분류기의 학습 결과를 바탕으로 다음 분류기의 학습 데이터의 샘플 가중치를 조정하여 학습하는 방법
- 틀린 샘플에 대해 가중치를 주고, 다음 훈련에도 샘플로 사용되도록 함.
- 동일한 알고리즘의 분류기를 순차적으로 학습해서 여러 개의 분류기를 만든 후, 테스트 할 때 가중 투표를 통해 예측값을 결정
- Sequential. 순차적으로 학습하는 특징이 있음. 훈련을 거듭할 수록 더 강력한 모델이 만들어진다. 앞단에 있는 모델의 부족한 부분을 뒤의 모델이 보충해 가는 방식.
- 최종적인 분류기를 선택 할 때에도 학습이 잘된 분류기에 더 큰 가중치를 줘서 분류기 선택 시 영향을 끼치도록 한다.

4. 스태킹
- 

