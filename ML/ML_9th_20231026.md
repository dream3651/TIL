## SVM 알고리즘(Support vector machine)

- 데이터 분류를 위하여 마진(margin)이 최대가 되는 '결정 경계선(decision boundary)'을 찾아내는 머신러닝 방법
- 데이터가 부족한 상황에서도 사용 가능한 기법
- 연속형 값의 분류도 가능함. Regressor 있음.
- 어느 정도의 오류는 허용하면서 최적의 결과를 만들어 내는 결정경계선을 찾는 것이 목표
- 특성이 다양한 데이터 분류에 강함. 파라미터 조정으로 과대/과소적합 대응 가능
- 데이터 전처리 과정이 매우 중요하며, 특성이 많을 경우, 결정경계 및 데이터의 시각화가 어려움. 고차원 데이터라면 저차원으로 옮겨서 시각화.
1. 결정경계선은 초평면(hyper plane)이라고도 함. 서로 다른 분류 값을 결정하는 경계. 선형뿐만 아니라 비선형모델도 가능함.
- 평면에서는 데이터를 구분할 때 선으로 가능하나, 다차원인 경우에는 2차원 면으로 구분해야 함. 차원이 증가할 수록 결정경계선의 차원도 올라가게 됨. 그래서 초평면이라고 함. 데이터의 벡터 공간이 n차원이라면, 결정경계선은 n-1 차원임.
2. 서포트 벡터: 벡터는 2차원 공간 상에 나타난 데이터 포인트를 의미. 서포트 벡터는 결정경계선과 가장 가까이 맞닿은 데이터 포인트.
3. 마진: 서포트 벡터와 결정경계 사이의 거리. 마진을 최대로 할 수 있는 결정경계선을 찾는 것이 목표이 SVM의 목표. 2차원에서 결정 경계선을 그리려면 서포트 벡터는 최소 3개가 필요함. n차원이면 최소 n+1개가 필요함.
4. 비용(cost): 얼마나 많은 데이터 샘플이 다른 클래스에 놓이는 것을 허용하는 지를 결정하는 것. 비용이 낮을수록 마진을 최대한 높이고, 학습 에러율을 증가시키는 방향으로 결정경계선을 만듦. 비용이 높을수록 마진은 작아지고, 학습 에러율은 감소하는 방향으로 결정경계선을 만듦(hard margin). 비용이 너무 낮으면 과소적합, 너무 높으면 과대적합의 위험이 있음(soft margin).
5. 커널 트릭(Kernel trick) : 주어진 차원 공간에서 구분이 안되는 데이터를 고차원 공간으로 옮겨서 데이터를 나타내어 분류하는 방법. 
- 선형SVM: 커널을 사용하지 않고 데이터를 분류. 비용을 조절해서 마진을 크기를 조정하는 모델을 설계함
- 커널트릭: 선형 분리가 주어진 차원에서 불가능할 경우, 고차원으로 데이터를 옮기는 효과를 통해 결정경계를 찾음. 이때 비용과 gamma를 조절해서 마진을 조절할 수 있다. 가우시안 RBF 커널 또는 RBF 모델이라고 함. 비용과 gamma로 커널 둘레의 길이를 조정하는 옵션. gamma는 표준편차 조정 변수임. gamma가 커질수록 경계가 데이터에 근접하게 되는 결과가 나온다. gamma는 결정경계의 구부러지는 정도(곡률)와 관련된 파라미터. 비용이 크면 클수록 마진의 너비가 좁아짐. 이런 모델은 과적합 위험이 있음. 일반화가 잘 되도록 조정하는 것이 중요함. 그리드서치 필수.


## 나이브베이즈 알고리즘
- 20세기 초반에 사용됐으나, 성능이 좋지 않아 지금은 잘 쓰이지 않음. 여기에서 파생된 알고리즘을 이해하기 위해 기본만 숙지
- 데이터를 나이브(단순)하게 독립적인 사건으로 가정하고, 독립 사건들을 베이즈 이론에 대입시켜 가장 높은 확률의 레이블로 분류를 실행하는 알고리즘. 


## 선형회귀 알고리즘

- 딥러닝의 시작. 가장 기본이 되는 모델이므로 잘 알아둬야 함.
- 회귀: 예상하지 못한 현상이 일어나더라도 그 현상은 결국 어떤 특정 예상 값으로 돌아간다는 의미. 두 변수의 관계가 어던 일반화된 선형관계의 평균으로 돌아간다(회귀)는 의미
- 선형회귀: 관찰된 데이터들을 기반으로 하나의 함수(모델)을 구해서 관찰되지 않은 데이터의 값을 예측하는 것. 연속형 값을 예측하는 모델이 직선으로 생겼을 때 선형회귀모델이라고 함. 과거의 데이터를 가지고 규칙을 탐색하여, 이를 바탕을 미래를 예측한다.
- 선형회귀 모델: 회귀계수를 선형적으로 결합할 수 있는 모델을 의미. 회귀계수는 그 직선의 기울기. 독립변수가 0일떄의 종속변수가 바로 상수가 된다. 독립변수 값의 변화가 종속변수에 미치는 영향력의 크기를 나타내는 계수가 회귀계수. 
- 독립변수(원인)와 종속변수(결과)를 좌표로 그래프를 그리고, 그 좌표를 연결하는 선을 통해 예측 모델을 만들수 있음. 인과관계를 나타내는 것이 회귀식(회귀함수).
- 단일회귀분석: 독립변수 1개와 종속변수 1개로 만드는 회귀분석
- 다중회귀분석: 여러개의 독립변수와 한개의 종속변수로 만드는 회귀분석
- 선형회귀 분석은 독립변수와 종속변수가 모두 정규 분포일 때 잘 작동한다. 데이터에 정규성이 없다면 표준화를 통해 정규성을 띄도록 한 후에 모델일 만들면 성능이 좋아진다.
- 정규성의 평가 : 첨도와 왜도.
- 오차(error): 회귀모델이 예상한 값(함수의 선)과 실제 정답들(데이터포인트)과의 차이. 선 위쪽의 오차는 양수, 아래쪽은 음수이므로, 오차를 제곱해서 평균을 낸다. MSE(Mean squared error:평균제곱함수). RMSE(MSE에 루트를 씌운 값), RMSLE(RMSE에 로그를 취한 값). 오차가 적은 모델이 좋은 모델임.
- 우리가 찾고자 하는 회귀 모델이란, MSE(평균제곱오차)가 최소인 함수. 경사하강법을 이용해서 찾는다.
- 경사하강법(SGD): 함수의 기울기를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 거의 극 값에 이를 때까지 반복하는 방법. Eary stopping(조기중지). MSE가 cost임. 함수에서 learning rate(학습률: 알파. 보통 초기값으로 0.01~0.001을 줌)을 지정해줘야 하는 하이퍼파라미터임.

## 주성분 분석(Principle Component Analysis)

- 고차원의 데이터를 저차원(3차원 이하)의 데이터로 차원을 축소하는 알고리즘
- 차원의 저주. 고차원일수록 공간낭비 & 계산량 증폭
- 고차원을 저차원으로 만드는 방법: 차원축소(Feature reduction)는 일종의 압축, 차원선택(Feature selection)은 선별(일부는 제거)
- 주성분 분석은 차원축소로, 데이터가 가지고 있는 의미를 최대한 살리면서 압축하는 기법
- 데이터의 분산은 최대한 유지하면서 저차원으로 데이터를 변환함. 데이터의 고유한 특성을 최대한 유지하기 위해 분산을 유지하려고 함. 분산이 커야 구분(식별)이 잘 된다. 데이터의 손실의 최소화. 정보의 유실이 가장 적은 1차원 직선을 찾아내는 것이 목표. 이선을 PCA 축이라고 함. 이 축과 직교하는 축을 그리면, 기존 데이터가 거의 복원됨.
- 몇 차원으로 축소할 때가 자료 손실이 최소인지를 확인하면서 테스트를 진행해야 함.


선형회귀모델 -> 로지스틱회귀모델 -> 소프트맥스(다중분류)-> 신경망

 


