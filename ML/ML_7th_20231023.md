## 복습
## Boosting
- 이전 분류기의 학습 결과를 바탕으로 다음 분류기의 학습 데이터의 샘플 가중치를 조정하여 학습하는 방법
- 동일한 알고리즘의 분류기를 순차적으로 학습해서 여러 개의 분류기를 만든 후, 테스트 할 때 가중 투표를 통해 예측값을 결정
- cf. 배깅: 병렬적 학습, 부스팅: 순차적 학습
- CART 모델 사용
- XGBoosting(Level-wise tree growth), LightGBM(Leaf-wise tree growth. XGBoosting보다 속도가 빠르고 더 가볍다. 단, 데이터가 1만건 이하의 경우, 과적합에 빠질 위험이 있다.), ADABoost, GradientBoost

- 앙상블 기법: 정형 데이터에 적용 적합. 
- Bagging: 같은 알고리즘으로 학습된 분류기를 만듦
- Voting: 서로 다른 알고리즘으로 학습된 분류기를 연결

- 편향과 분산이 적은 모델이 좋은 모델


4. Stacking
- 일발적인 모델 생성, 테스트 과정과 좀 다름
- train 데이터로 다양한 각각의 모델을 훈련시킴. 그 결과로 나온 예측값들을 다시 훈련 데이터로 사용하는 방식
- Vanila stacking기법은 과적합 우려가 있음.
- CV를 기반(k-fold)으로 하는 stacking기법이 많이 사용됨.


## Linear Regression(선형회귀) 모델
- 연속형 데이터에서 사용
- 선형모델을 만들 때는, 데이터가 선형적 형태를 띄고 있다고 가정함
- H(x) = Wx + b
- W와 b가 어떤 값을 가질 때, 데이터의 분포를 가장 잘 드러내는가
- 직선과 점 사이의 거리가 가장 짧은 직선이 좋은 함수이다
- 직선과 점 사이의 거리: cost function
- cost값이 크다: 직선과 점 사이의 거리가 크다. 멀리 떨어져 있다
- cost값이 작다: 직선과 점 사이의 거리가 작다. 가깝게 있다.
- H(x) - y = Error
- Error를 작게 하는 값이 좋은 함수임
- cost가 작은 모델이 좋은 모델임. Cost를 최소로하는 W와 b를 찾는 것이 목표. 어떻게 찾는가?
- Gradient descent algorithm(경사하강법): cost가 가장 낮은 지점의 w를 찾아냄. 최소화 문제에 많이 사용되는 방법. 다차원 데이터에서도 사용 가능함. 
- 임의의 초기 W와 b값을 부여 -> 조금씩 변경해가면서 cost 값을 감소 시킴 -> local minimum(cost가 가장 낮은 지점)을 찾을 때 까지 반복

## Logistic classification ( = Logistic regression)
- 경사하강법으로는 연속형 값을 예측할 수 있는 모델을 만들 수 있음.
- 이번에는 0 또는 1로 분류 작업을 할 때 linear regression 모델을 적용하는 방법
- 크래디트 카드 불법 거래 감지 가능
- Linear regression은 H(x)가 마이너스 무한대~플러스 무한대로 출력됨.
- 하지만, 우리의 가설은 H(x)가 0 또는 1로 출력되어야 함.
- 이런 값으로 출력해주는 함수가 시그모이드 함수(sigmoid function).
- Gradient decent 알고리즘을 그대로 적용하나, cost 함수만 변경해서 사용하면 됨.
- 이진분류가 가능해짐

## Softmax Cross-Entrop
- 다중분류(multinomial clasification)에서 사용되는 함수
- 클래스의 종류의 개수만큼 분류기를 만들면 가능함.
- 거리로 분류를 판단. 좀 더 가까운 쪽으로 분류.
- 

