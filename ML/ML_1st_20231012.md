# '혼자공부하는 머신러닝+딥러닝' 책 소개

## 머신러닝

1. 기계가 지능을 갖도록 학습시키는 방법 (학습 목적 a. 분류 b. 예측)

2. 모델 평가
- 어느 정도의 지능을 갖추고 있는 지 객관적으로 평가함. 기계가 학습할 때 사용한 데이터가 아닌 새로운 데이터를 가지고 평가를 진행한다.

3. 데이터의 분류
- 훈련 데이터 셋과 테스트 데이터 셋이 필요함. 훈련 데이터로 학습을 시키면 모델이 만들어짐. 이 모델의 성능을 판단할 때 테스트 데이터 셋을 사용함.

4. 데이터 전처리

5. 회귀 알고리즘과 모델 규제: 예측의 문제를 해결하기 위한 알고리즘. *알고리즘? 문제를 해결하는 방법

6. 로지스틱 회귀: 회귀 알고리즘에서 파생된 분류 알고리즘

7. 의사결정 트리 알고리즘
- 나온 지 오래되어 알고리즘의 완성도(결함이 거의 없음)가 높지만, 정확도가 낮은 단점이 있다. 2000년대 초반까지 많이 쓰였음. 여기서 파생된 응용 알고리즘이 많음. 다수의 트리가 모여 만들어진 알고리즘이  많다. 랜덤 포레스트, 그레이디언트 부스팅, 스태킹 등

8. 교차 검증
- 모델을 평가하는 방법 중 하나. cross valuation(CV)

9. 그리드 서치
- 더 좋은 모델을 만들 수 있도록 도움을 주는 기능. 데이터에 따라 가장 적합하고 효율적인 최적의 알고리즘을 탐색해야 함. 최적의 알고리즘을 찾기 위해 다양한 알고리즘으로 테스트를 하고 성능 결과를 평가, 비교해야 함. - 여기서 선택한 알고리즘을 가지고 더 세부적으로 적합한 방법(하이퍼파라미터)을 자동으로 찾아주는 기능. 이 과정을 하이퍼파라미터 튜닝이라고 부른다.

10. 트리의 앙상블
- 트리를 여러개를 만들어 섞는 기법.

11. 군집 알고리즘
- 유사한 데이터끼리 묶여있는 상태를 군집(cluster)라고 부름. 클러스터를 만드는 알고리즘(Clustering algorithm). 클러스터에 대한 해석은 데이터 분석가가 해 줘야 함. h-clustering, k-means, k-means plus, db scanning

12. 주성분 분석(PCA: principle component analysis)
- 독립변수(x. 복수도 가능)와 종속변수(y). 독립변수에 개수에 따라 차원이 달라짐. 차원이 커지면 계산이 불가능하므로, 차원을 축소하여 연산을 쉽게 할 수 있다. 이런 방법을 PCA라고 함. 다만 차원을 줄이면, 데이터에 내재되어 있는 정보의 손실이 생기게 됨.

## 딥러닝

1. 딥러닝? 
- 깊은 신경망. 
- 초기의 얕은 신경망은 구조적 한계로 성능이 좋지 않았음. 오히려 SVM이나 랜덤 포레스트의 방법이 더 많이 사용되었음. 그러다가 성능 개선의 노력의 결과로, 2006년 몬트리올 대학교 힌튼 교수가 발표한 논문으로 딥러닝이 처음 소개 됨. 상당히 성능이 좋음. 
- 우리나라에서는 2010년경 부터 연구 시작됨. 엄밀히 얘기하면 딥러닝은 머신러닝 알고리즘의 하나.
- 신경망은 입력계층, 히든계층, 출력계층으로 구성. 히든계층이 하나면 얕은 신경망, 두개  이상있으면 깊은 신경망이라고 함.

3. 최적화 모델 만들기
- 오버피팅: 데이터를 통해 학습시켜(트레이닝) 모델 생성하는 과정에서, 데이터가 가지고 있는 특성을 너무 과하게 학습하게 되면 오버피팅이 발생한다. 이를 줄이기 위해서 손실 곡선, 검증 손실, 드롭 아웃 등이 있음. 

4. 심층 신경망
- 깊은 신경망. 초창기에는 이미지 분류에 많이 사용됨. 스탠포드 대학 엔드류 융 교수가, 이미지 분류를 더 잘할 수 있는 심층 신경망을 발표. 이미지 넷 대회. 딥러닝 전문가 콜라(Colah). 합성곱신경망 구조. 
- CNN 구조(이미지 분류)
- RNN 구조(예측. recurrent neural network)는 데이터가 시계열데이터인 경우에 적합한 네트워크 구조. 텍스트 생성, 미래 예측에 사용됨.
- 하지만 CNN을 예측에 쓸 수도 있고, RNN을 분류에 쓸 수도 있음.
- LSTM 네트워크: RNN의 장기 의존성 문제점을 다소 해결함. 
- GRU: RNN의 장기 의존성 문제를 다소 해결함.
- Sequence to sequence, attention

5. [HTML](https://scikit-learn.org/)
- 스킷런. 가장 좋은 머신러닝 딥러닝의 메뉴얼

6. TensorFlow
- 처음에 공부하기 상당히 어렵다.
- 딥러닝 플랫폼 캐라스를 이용해 TensorFlow를 좀 더 쉽게 사용 가능.
- TensorFlow보다 PyTorch 프레임웍도 많이 씀

## LG데이터 전처리(ppt)
- 데이터 마이닝? 대규모의 데이터를 다 검사할 수 없으므로, 일부의 데이터를 가지고 검사하는 것. 대규모의 데이터에서 일장한 규칙을 찾아내는 것을 말한다. 데이터 마이닝에 머신러닝이 들어간다고 볼 수 있음.

## 딥러닝 기초 (ppt)

## 통합본_편집본 (ppt)
- 머신러닝

## 실습 환경
- 구글 코랩


# 1. 머신러닝과 친해지기

## 머신러닝의 정의
1. 머신러닝과 전통적 프로그래밍의 차이
- 다항시간(polynomial time): 어떤 문제를 계산함에 있어서 주어진 시간 안에 답이 나오는 시간. 
- 전통적 프로그래밍 언어에서는 NP문제를 풀 수 없다.
- 인공지능에서는 확률로 접근하여 해결 가능함.
- 교사학습(지도학습)
- 특정 알고리즘을 가지고 학습을 통하여 어느정도 지식이 쌓이 상태를 모델이라고 함.
- 데이터의 학습을 통해 확률적인 방법으로 해답을 추론해 줌. 분류모델.
- 연속형 수치데이터를 입력하여 결과를 예측하도록 함 예측모델.

## 머신러닝의 활용 분야
1. 영상인식: OCR, Object detection (CNN, YOLO, 혹은 이를 이용한 사용자 기반 모델도 만들 수 있음)
2. 문자인식
3. 얼굴인식
4. 음성인식
5. 자연어 처리
6. 정보 검색: 스팸 메일 필터링
7. 검색 엔진: 개인 맞춤형 추천 시스템. 협업 필터링 알고리즘, 아이템 기반/유저 기반/하이브리드 추천 시스템 등. 
8. 로보틱스

## 머신러닝과 인공지능의 관계
1. 지도학습. 교사가 있으면 지도학습. 신경망(딥러닝)은 지도학습에 포함됨.
2. 비지도학습. 교사가 없으면 비지도학습.
3. 강화학습. 시행착오를 통해 모델이 강화됨.

- 지도학습(supervised learning): 정답을 알려주고 미리 학습을 시킴. 데이터의 특징, 패턴이 바로 모델이다. 모델링이란 데이터가 가진 특징을 찾는 과정이라고 할 수 있음. 분류(영상분류, 사기탐지, 진단, 번호판 인식)와 회귀(시장 예보, 인구증가 예측, 날씨 예측)
- 비지도학습(Unsupervised learning): 데이터의 특성을 스스로 학습하는 것. 답이 없어도 학습을 한다. 클러스터링을 사용하면 데이터의 특성을 바탕으로 데이터가 나뉘게 됨. 그 결과에 대해 항상 그 결과가 나온다고 보장할 수 없음. 클러스터링(목표 마케팅, 추천 시스템, 고객 세분화, 데이터 마이닝), 차원축소(빅데이터 가시화, 특징 추출).
- 강화학습(Reinforcement learning): 지속적인 '시행착오'를 통해 '보상'을 극대화하는 방법으로 학습이 진행됨. (실시간 판단- 선물거래, 로보어드바이저, 가상화폐, 로봇 네비게이션, 인공지능 게임, 학습 업무) 

## 유클리디안 거리(Euclidean distance)의 정의
- 2차원 데이터에서 각 점 사이의 거리 : (밑변 차이의 제곱 +높이 차이의 제곱)의 루트값. distance matrix. 이를 응용하여 n차원 데이터에서 각 점의 거리도 구할 수 있음. 거리가 가까울 수록 데이터가 서로 유사하다는 결론을 내릴 수 도 있음. 유사한 정도를 알기위해 가장 많이 사용됨. 
- 문제점: 거리 이외의 변수를 무시함. 의도와 다른 결과를 내놓을 수 도 있음.
- 해결점: 벡터간의 거리가 이루는 각도가 작을 수록 유사도가 높다고 판단. 각도가 0일 때 똑같다고 판단. 90도일때 전혀 다르다고 판단. 코사인 함수 값을 이용. => #코사인 유사도# 


